{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uawNySXlUO-P"
      },
      "source": [
        "# **Rock-Paper-Scissor Competition (40%)**\n",
        "For this competition, we will use the Game (https://cloudstor.aarnet.edu.au/plus/s/6QNijohkrfMZ0H7) dataset. This dataset contains images of hand gestures from the Rock-Paper-Scissors game. \n",
        "\n",
        "The dataset contains a total of 2188 images corresponding to the 'Rock' (726 images), 'Paper' (710 images) and 'Scissors' (752 images) hand gestures of the Rock-Paper-Scissors game. All image are taken on a green background with relatively consistent ligithing and white balance.\n",
        "\n",
        "All images are RGB images of 300 pixels wide by 200 pixels high in .png format. The images are separated in three sub-folders named 'rock', 'paper' and 'scissors' according to their respective class.\n",
        "\n",
        "The task is to categorize each hand guesters into one of three categories (Rock/Paper/Scissor). \n",
        "\n",
        "We provide a baseline by the following steps:\n",
        "\n",
        "\n",
        "*   Loding and Analysing the dataset using torchvision.\n",
        "*   Defining a simple convolutional neural network. \n",
        "*   How to use existing loss function for the model learning. \n",
        "*   Train the network on the training data. \n",
        "*   Test the trained network on the testing data. \n",
        "\n",
        "The following trick/tweak(s) could be considered:\n",
        "-------\n",
        "1. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, Number of Max Epochs, and Drop-out. \n",
        "2. Use of a new loss function.\n",
        "3. Data augmentation\n",
        "4. Architectural Changes: Batch Normalization, Residual layers, Attention Block, and other varients.\n",
        "\n",
        "Your code should be modified from the provided baseline. A pdf report is required to explain the tricks you employed, and the imporvements they achieved.\n",
        "Marking Rules:\n",
        "-------\n",
        "We will mark the competition based on the final test accuracy on testing images and your report.\n",
        "\n",
        "Final mark = acc_mark + efficiency mark + report mark + bonus mark\n",
        "###Acc_mark 15:\n",
        "\n",
        "We will rank all the submission results based on their test accuracy. The top 30% of the students will get full marks.\n",
        "\n",
        "\n",
        "|Accuracy|Mark|\n",
        "|---|---|\n",
        "| Top 30% in the class|          15|\n",
        "|30%-50%|         11|\n",
        "|50%-80%  |        7|\n",
        "| 80%-90%  |      3|\n",
        "| 90%-100%  |      1|\n",
        "|Not implemented| 0|\n",
        "\n",
        "###Efficiency mark 5:\n",
        "\n",
        "Efficiency is evaluated by the computational costs (flops: https://en.wikipedia.org/wiki/FLOPS). Please report the computational costs for your final model and attach the code/process about how you calculate it.\n",
        "\n",
        "|Efficiency|Mark|\n",
        "|---|---|\n",
        "| Top 30% in the class|          5|\n",
        "|30%-50%|         4|\n",
        "|50%-80%  |        3|\n",
        "| 80%-90%  |      2|\n",
        "| 90%-100%  |      2|\n",
        "|Not implemented| 0|\n",
        "\n",
        "###Report mark 20:\n",
        "1. Introduction and your understanding to the baseline model: 2 points\n",
        "\n",
        "2. Employed more than three tricks with ablation studies to improve the accuracy: 6 points\n",
        "\n",
        "Clearly explain the reference, motivation and design choice for each trick/tweak(s). Providing the experimental results in tables.\n",
        "Example table:\n",
        "\n",
        "|Trick1|Trick2|Trick3|Accuracy|\n",
        "|---|---|---|---|\n",
        "|N|N|N|60%|\n",
        "|Y|N|N|65%|\n",
        "|Y|Y|N|77%|\n",
        "|Y|Y|Y|82%|\n",
        "\n",
        "Observation and discussion based on the experiment results.\n",
        "\n",
        "3. Expaination of the methods on reducing the computational cost and/or improve the trade-off between accuracy and efficiency: 4 points\n",
        "\n",
        "4. Explaination of the code implementationï¼š3 points\n",
        "\n",
        "5. Visulization results: e.g. training and testing accuracy/loss for each model, case studies: 3 points\n",
        "\n",
        "6. Open ended:  Limitations, conclusions, failure cases analysis...: 2 points\n",
        "\n",
        "###Bouns mark:\n",
        "1. Top three results: 2 points\n",
        "2. Fancy designs: 2 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wHkPrQfVhPI"
      },
      "outputs": [],
      "source": [
        "##################################################################################################################################\n",
        "### Subject: Computer Vision \n",
        "### Year: 2022\n",
        "### Student Name: Yuanxi Wang, Germin Chan\n",
        "### Student ID: a1805637, a1805312\n",
        "### Competiton Name: Rock-Paper-Scissor Classification Competition\n",
        "### Final Results:\n",
        "### ACC:0.9995659589767456    FLOPs:0.41G\n",
        "##################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAea9HHKomad"
      },
      "outputs": [],
      "source": [
        "# Importing libraries. \n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# To avoid non-essential warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torchvision import datasets, transforms, models \n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mkadj8e1sCQS",
        "outputId": "02a91b31-c2e3-46be-9f75-ea9aeb377cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting G-Drive to get your dataset. \n",
        "# To access Google Colab GPU; Go To: Edit >>> Network Settings >>> Hardware Accelarator: Select GPU. \n",
        "# Reference: https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount = True)\n",
        "\n",
        "# Dataset path.\n",
        "data_dir = '/content/drive/MyDrive/gamex5'\n",
        "classes = os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5F9VBdNcsGEI"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms\n",
        "# Performing Image Transformations.\n",
        "##Hints: Data Augmentation can be applied here. Have a look on RandomFlip, RandomRotation... \n",
        "train_transform=transforms.Compose([\n",
        "\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomVerticalFlip(0.5),\n",
        "        transforms.Grayscale(1),\n",
        "\n",
        "        transforms.Resize(24),             # resize shortest side Hints: larger input size can lead to higher performance\n",
        "        transforms.CenterCrop(24),         # crop longest side Hints: crop size is usuallt smaller than the resize size\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406],\n",
        "        #                      [0.229, 0.224, 0.225])\n",
        "        transforms.Normalize((0.5),(0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1lgLwj4VsH5y",
        "outputId": "7d959807-b865-45f7-bcbb-956b44cdff2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training dataset : 10940\n"
          ]
        }
      ],
      "source": [
        "# Checking the dataset training size.\n",
        "dataset = ImageFolder(data_dir, transform=train_transform)\n",
        "print('Size of training dataset :', len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C_W_Uf_msJny",
        "outputId": "ad278215-2de7-4c86-df18-43f496dd4d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 24, 24])\n"
          ]
        }
      ],
      "source": [
        "# Viewing one of images shape.\n",
        "img, label = dataset[100]\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6UTOv1u_sLbu"
      },
      "outputs": [],
      "source": [
        "# Preview one of the images..\n",
        "def show_image(img, label):\n",
        "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
        "    # img.permute(1,2,0)\n",
        "    plt.imshow(img[0,:,:],cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eQzaOFkKsNGu",
        "outputId": "529c8ec4-eb91-4eb1-b22c-7eb9f09f1fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label:  paper (0)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_image(*dataset[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VRXOQ-_4sO2L"
      },
      "outputs": [],
      "source": [
        "# Setting seed so that value won't change everytime. \n",
        "# Splitting the dataset to training, validation, and testing category.\n",
        "torch.manual_seed(10)\n",
        "val_size = len(dataset)//10\n",
        "test_size = len(dataset)//5\n",
        "train_size = len(dataset) - val_size - test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "StEh6Ea0sRLF",
        "outputId": "6999d0aa-85c5-4c55-f0cf-7226ab02bed8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7658, 1094, 2188)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random Splitting. \n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_ds), len(val_ds),len(test_ds)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e9IucnuKsd8s"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size*2, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ75Us1PsiS0"
      },
      "outputs": [],
      "source": [
        "# Multiple images preview. \n",
        "for images, labels in train_loader:\n",
        "    fig, ax = plt.subplots(figsize=(18,10))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Aj84xVnsmVj"
      },
      "outputs": [],
      "source": [
        " # Baseline model class for training and validation purpose. Evaluation metric function - Accuracy.\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8z5xrLbspAp"
      },
      "outputs": [],
      "source": [
        "# Functions for evaluation and training.\n",
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCCgOb3Ksr6T"
      },
      "outputs": [],
      "source": [
        "# To check wether Google Colab GPU has been assigned/not. \n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Y321w9suir"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XvdR0GZsv82"
      },
      "outputs": [],
      "source": [
        "input_size = 1*24*24\n",
        "output_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tl6gXw1s2uT"
      },
      "outputs": [],
      "source": [
        "# Model - 7 Layer\n",
        "class CnnModel(ImageClassificationBase):\n",
        "    def __init__(self, classes):\n",
        "        super().__init__()\n",
        "        self.classes = classes\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 100, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(100, 150, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), \n",
        "\n",
        "            nn.Conv2d(150, 200, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), \n",
        "\n",
        "            nn.Conv2d(200, 250, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(250, 250, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), \n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(2250, 64),  \n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(64, 32),  \n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(32, 16),           \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(8, self.classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crPVXO__s4Rn"
      },
      "outputs": [],
      "source": [
        "# Model print\n",
        "num_classes = 3\n",
        "model = CnnModel(num_classes)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIRKQ8sKs-kD"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_loader:\n",
        "    out = model(images)\n",
        "    print('images.shape:', images.shape)    \n",
        "    print('out.shape:', out.shape)\n",
        "    print('out[0]:', out[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfL7daKDtB1u"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(val_loader, device)\n",
        "to_device(model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhK2IHrftEHo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xXrYtotGpP"
      },
      "outputs": [],
      "source": [
        "model = to_device(CnnModel(num_classes), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnNp6XbftIhR"
      },
      "outputs": [],
      "source": [
        "history=[evaluate(model, val_loader)]\n",
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbp2a47DtKFp"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zVFA8NktLgV"
      },
      "outputs": [],
      "source": [
        "history += fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW0OMYMxtN1X"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JupvRPgCtPJ3"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history)\n",
        "plot_losses(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtTNagzaJKct"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYPTk-ayBpaE"
      },
      "outputs": [],
      "source": [
        "# torch.save(model, '/content/drive/MyDrive/results/datasetx5all3transforms3gray24x24.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsh4oHm2hMhp"
      },
      "source": [
        "##FLOPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-effbw3hOX0"
      },
      "outputs": [],
      "source": [
        " #The code from https://cloudstor.aarnet.edu.au/plus/s/PcSc67ZncTSQP0E can be used to count flops\n",
        "#Download the code.\n",
        "!wget -c https://cloudstor.aarnet.edu.au/plus/s/hXo1dK9SZqiEVn9/download\n",
        "!mv download FLOPs_counter.py\n",
        "#!rm -rf download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YDiWRfkhSSj"
      },
      "outputs": [],
      "source": [
        "from FLOPs_counter import print_model_parm_flops\n",
        "input = torch.randn(1, 1, 24, 24) # The input size should be the same as the size that you put into your model \n",
        "#Get the network and its FLOPs\n",
        "num_classes = 3\n",
        "model = CnnModel(num_classes)\n",
        "print_model_parm_flops(model, input, detail=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX3RuVRYBjrv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}